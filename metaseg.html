<!doctype html>
<html lang="en">
<head>

<meta charset="UTF-8">

		<meta name="viewport" content="width=device-width, initial-scale=1.0">
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-EL03DLW9KF"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'G-EL03DLW9KF');
        </script>


        <link rel="stylesheet" type="text/css" href="https://kushalvyas.github.io/theme/css/bootstrap.min.css">
        <script src="https://kushalvyas.github.io/theme/js/bootstrap.min.js"></script>
        <link rel="stylesheet" type="text/css" href="https://kushalvyas.github.io/theme/fontawesome/css/fontawesome.min.css">
        <link rel="stylesheet" type="text/css" href="https://kushalvyas.github.io/theme/css/styles.css">
        <script src=
        "https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js">
            </script>
            <script src=
        "https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js">
            </script>
        <link href='http://fonts.googleapis.com/css?family=Roboto' rel='stylesheet' type='text/css'>
        <style>
            .navbar-nav {
                margin-left: auto;
            };
            .jumbotron_class{
                background-image: "background.jpeg";
            };
            body{
                font-family: 'Roboto', sans-serif;
            }
            
        </style>

        <script>
            $("document").ready(function(){
                $("#backdrop_image").click(function(e){
                    e.preventDefault();
                });

                // $("#page_banner").click(function(e){
                //     window.location.href = "./"
                // });
            });
        </script>

         <script src="https://kushalvyas.github.io/theme/js/photoswipe.umd.min.js"></script>
        <script src="https://kushalvyas.github.io/theme/js/photoswipe-lightbox.umd.min.js"></script>
        <link rel="stylesheet" href="https://kushalvyas.github.io/theme/css/photoswipe.css">
        <script src="https://kushalvyas.github.io/theme/js/scramble.js"></script>

        <style>
            li {
                margin: 0;
                padding: 0em;
            }
        </style>

        <script src="https://kushalvyas.github.io/theme/js/spotlight.bundle.js"></script>   
</head>
<body>
    <div class="container">
<div id="page_header">
    <nav class="navbar navbar-expand-md bg-light">
        
        <a class="nav-link" href="./">&nbsp; HOME &#124;	 &nbsp; </a>
        <a class="navbar-brand abs" href="https://www.linkedin.com/in/kushalvyaskv/"><svg xmlns="http://www.w3.org/2000/svg" width="1.5em" height="24" viewBox="0 0 24 24"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/></svg></a>
        
        <a class="navbar-brand abs" href="https://github.com/kushalvyas"><svg xmlns="http://www.w3.org/2000/svg" height="1.5em" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) Copyright 2023 Fonticons, Inc. --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></a>
        
        <a class="navbar-brand abs" href="https://scholar.google.com/citations?user=0SxLnLcAAAAJ&hl=en"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" height="1.5em"><!--!Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free Copyright 2024 Fonticons, Inc.--><path d="M390.9 298.5c0 0 0 .1 .1 .1c9.2 19.4 14.4 41.1 14.4 64C405.3 445.1 338.5 512 256 512s-149.3-66.9-149.3-149.3c0-22.9 5.2-44.6 14.4-64h0c1.7-3.6 3.6-7.2 5.6-10.7c4.4-7.6 9.4-14.7 15-21.3c27.4-32.6 68.5-53.3 114.4-53.3c33.6 0 64.6 11.1 89.6 29.9c9.1 6.9 17.4 14.7 24.8 23.5c5.6 6.6 10.6 13.8 15 21.3c2 3.4 3.8 7 5.5 10.5zm26.4-18.8c-30.1-58.4-91-98.4-161.3-98.4s-131.2 40-161.3 98.4L0 202.7 256 0 512 202.7l-94.7 77.1z"/></svg></a>


        <!-- <a class="navbar-brand abs"></a>     -->

        <ul class="navbar-nav ms-auto navbar-right">
            <li class="nav-item">
              <a class="nav-link" href="https://kushalvyas.github.io/research.html">
                Research
              </a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="https://kushalvyas.github.io/photography.html">
                  Photography & Music
                </a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="https://kushalvyas.github.io/blog.html">
                  Blog
                </a>
            </li>
        </ul>
    </nav>

</div>        <div id="page_banner">
<div>
    <!-- Jumbotron -->
    <div class="p-5 text-center bg-image rounded-3">
      <div class="mask" style="background-color: rgba(0, 0, 0, 0.6);">
        <div class="d-flex justify-content-center align-items-center h-100">
          <div class="text-white">
            <h1 class="mb-3">Fit pixels, get labels: Meta-learned Implicit Networks for Image Segmentation (MetaSeg)</h1>
            <h6 class="mb-3">
                <a href="https://kushalvyas.github.io/" style="color: white;"><b>Kushal Vyas</b></a>
                <a href="https://profiles.rice.edu/faculty/ashok-veeraraghavan" style="color: white;">Ashok Veeraraghavan</a>
                <a href="https://www.guhabalakrishnan.com/home" style="color: white;">Guha Balakrishnan</a>
                <p><i>MICCAI (Best Paper Award, ORAL Presentation), 2025</i></p>
            </h6>
            
          </div>
        </div>
      </div>
    </div>
    <!-- Jumbotron -->
    </div>        </div>
        
<div class="container">
    <div class="justify-content-center">
    <article>
            <br>
            <div class="row justify-content-md-center">
                <div class="col-md-auto box text-center">
                    <div>
                        <img src="projects/images/metaseg/arxiv_thumbnail.png" style="width: 100px; height: 100px; object-fit: cover;">
                        <p><a href="https://papers.miccai.org/miccai-2025/0340-Paper3113.html">Paper link</a></p>
                    </div>
                </div>
                <div class="col-md-auto box text-center">
                    <div>
                        <img src="projects/images/metaseg/colab_thumbnail.png" style="width: 100px; height: 100px; object-fit: cover;">
                        <p><a href="https://colab.research.google.com/drive/1C9xon4HPBtXA_GTxPuSNRlTIU853qbIt?usp=sharing">Google Colab</a></p>
                    </div>
                </div>
                <div class="col-md-auto box text-center">
                    <div>
                        <img src="projects/images/metaseg/github_thumbnail.png" style="width: 100px; height: 100px; object-fit: cover;">
                        <p><a href="https://github.com/kushalvyas/MetaSeg">Code</a></p>
                    </div>
                </div>
            </div>
        </div>
            <br>
        <div class="justify-content-center" style="text-align: justify;">
            <p><strong>Abstract</strong>: Implicit neural representations (INRs) have achieved remarkable successes in learning expressive yet compact signal representations. However, they are not naturally amenable to predictive tasks such as segmentation, where they must learn semantic structures over a distribution of signals. In this study, we introduce <b>MetaSeg</b>, a meta-learning framework to train INRs for medical image segmentation. <b>MetaSeg</b> uses an underlying INR that simultaneously predicts per pixel intensity values and class labels. It then uses a meta-learning procedure to find optimal initial parameters for this INR over a training dataset of images and segmentation maps, such that the INR can simply be fine-tuned to fit pixels of an unseen test image, and automatically decode its class labels. We evaluated <b>MetaSeg</b> on 2D and 3D brain MRI segmentation tasks and report Dice scores comparable to commonly used U-Net models, but with <span class="math">\(90\%\)</span> fewer parameters. <b>MetaSeg</b> offers a fresh, scalable alternative to traditional resource-heavy architectures such as U-Nets and vision transformers for medical image segmentation.</p>
<p><br>
<br></p>
<figure align="center">
  <img src="projects/images/metaseg/metaseg_coverfigure.png" alt="metaseg cover fig" width="80%"/>
  <figcaption align="center">
    <b>Overview of MetaSeg.</b>  
    </figcaption>
</figure>
<p>We use a meta-learning framework (shown in (a)) to learn optimal initial parameters <span class="math">\(\theta^*, \phi^*\)</span> for an INR consisting of an <span class="math">\(L\)</span>-layer reconstruction network <span class="math">\(f_\theta(\cdot)\)</span> and shallow segmentation head <span class="math">\(g_\phi(\cdot)\)</span>.  At test time (b), optimally initialized INR <span class="math">\(f_{\theta^*}\)</span> is iteratively fit to the pixels of an unseen test scan. After convergence, the penultimate features <span class="math">\(f_{\theta^*}^{L-1}(x)\)</span> are fed as input to the segmentation head <span class="math">\(g_{\phi^*}(\cdot)\)</span> to predict per-pixel class labels.</p>
<p><br>
<br></p>
<h4>MetaSeg Architecture</h4>
<figure align="center">
  <img src="projects/images/metaseg/metaseg_architecture.png" alt="metaseg arch fig" width="70%"/>
  <figcaption align="center">
    MetaSeg architecture.
  </figcaption>
</figure>
<p>We use a Siren-INR where we share the initial layers of the INR are shared across a reconstruction head that fits the signal and a segmentation head which decodes learned INR features into per-pixel segmentation maps. We learn a semantically generalizable INR initialization for the INR and segmentation head using a meta-learning framework <em>(</em>Section 2 of paper)*. At test time, we fit the INR to the pixels of an unseen image and use the learned features to predict segmentation maps.</p>
<p><br>
<br></p>
<h4>Test-time: Fit pixels, get labels!</h4>
<figure align="center">
  <img src="projects/images/metaseg/metaseg_progression.png" alt="metaseg test time fig" width="70%"/>
  <figcaption align="center">
    MetaSeg Test-time fitting progression.
  </figcaption>
</figure>

<p>At test-time, MetaSeg initialized INRs along with its frozen segmentation head iteratively <strong>only fit the signal (intensity)</strong> while the segmentation head simultaneously decodes learned INR features of each pixel into a per-pixel segmentation map. it&rsquo;s that simple: Fit pixels, to get labels! Further, thanks to the meta learned initalization, MetaSeg fits the signal faster and better than a randomly initialized INR <em>(refer ablation table and discussion section in paper)</em>.</p>
<p><br>
<br></p>
<h4> MetaSeg achieves comparable segmentation to U-Nets with 90% fewer parameters</h4>
<figure align="center">
  <img src="projects/images/metaseg/metaseg_results_table.png" alt="metaseg results fig" width="80%"/>
  <figcaption align="center">
    MetaSeg evaluated on OASIS-MRI dataset
  </figcaption>
</figure>

<h4>Generalization of MetaSeg to small and large structures</h4>
<figure align="center">
  <img src="projects/images/metaseg/small_large_structures.png" alt="metaseg structures fig" width="80%"/>
  <figcaption align="center">
    Qualitative visualization of MetaSeg segmentation on varying brain anatomies.
  </figcaption>
</figure>
<p>We further depict qualitatively, that MetaSeg&rsquo;s semantic initialization generalizes well to varying brain anatomies, accurately accommodating large and small structures. Here we observe that high structural variances shown in the grey matter and brain stem are easily captured by MetaSeg. Deeper and delicate structures such as the hippocampus and ventricles (which can be a few pixels wide) are also segmented accurately by MetaSeg.</p>
<h4>High quality 3D segmentation and generating cross-sectionv views with MetaSeg</h4>
<figure align="center">
  <img src="projects/images/metaseg/metaseg3d.png" alt="metaseg3d fig" width="80%"/>
  <figcaption align="center">
    MetaSeg 3D segmentation and cross-section generation for 3D T-1 MRI scans.
  </figcaption>
</figure>
<p>MetaSeg also scales to 3D signals readily and yields high quality 3d segmentation by only fitting 3D MRI scans. Further, since INRs are naturally interpolating models, we can query MetaSeg to obtain the segmentation of any section of the brain. As shown, MetaSeg can generate high quality cross sections with a dice of 0.93, which is in high agreement with the ground truth</p>
<h4>Analysis of learned MetaSeg features</h4>
<figure align="center">
  <img src="projects/images/metaseg/pca_features.png" alt="metaseg pca fig" width="80%"/>
  <figcaption align="center">
    MetaSeg architecture.
  </figcaption>
</figure>
<p>MetaSeg has shown impressive performance on 2D and 3D segmentation. To further understand the benefits of MetaSeg’s initialization and design, we visualize the principal components of MetaSeg’s learned INR feature space. We find that unlike a typical INR which would only fit to intensity, yielding seeming random deep principal components, MetaSeg’s feature space embeds semantic regions. As seen in components 3-5, regions such as ventricles, hippocampus, and white-grey matter boundary is strongly encoded in MetaSeg&rsquo;s feature space.</p>
<p><br>
<br></p>
<p><strong>For more details, please refer <a href="https://papers.miccai.org/miccai-2025/paper/3113_paper.pdf">full paper</a>!</strong></p>
<p><br></p>
<h2 id="citation"><strong>Citation</strong><a class="headerlink" href="#citation" title="Permanent link">&para;</a></h2>
<div class="highlight"><pre><span></span><code><span class="w">            </span><span class="nv">@InProceedings</span><span class="err">{</span><span class="n">vyas2025metaseg</span><span class="p">,</span>
<span class="w">                </span><span class="n">author</span><span class="o">=</span><span class="ss">&quot;Vyas, Kushal</span>
<span class="ss">                    and Veeraraghavan, Ashok</span>
<span class="ss">                    and Balakrishnan, Guha&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">title</span><span class="o">=</span><span class="ss">&quot;Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">booktitle</span><span class="o">=</span><span class="ss">&quot;Medical Image Computing and Computer Assisted Intervention -- MICCAI 2025&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="nf">year</span><span class="o">=</span><span class="ss">&quot;2026&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">publisher</span><span class="o">=</span><span class="ss">&quot;Springer Nature Switzerland&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">pages</span><span class="o">=</span><span class="ss">&quot;194--203&quot;</span><span class="p">,</span>
<span class="w">                </span><span class="n">isbn</span><span class="o">=</span><span class="ss">&quot;978-3-032-04947-6&quot;</span>
<span class="w">            </span><span class="err">}</span>
</code></pre></div>

<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </div>
    </article>
    </div>
</div>
        
    </div>

<footer class="bg-light text-center text-lg-start">

    <div class="text-center p-3" style="background-color: rgba(248,249,250);">
      Created this website using Python, Pelican, Markdown, Jinja, Bootstrap, JQuery, FontAwesome, MDBootstrap
    </div>

</footer>

<!-- <footer style="background-color: #f8f9fa; text-align: center; margin-top: auto; position: sticky; top: 100vh;">
  <div style="padding: 1rem; background-color: rgba(248,249,250);">
    Created this website using Python, Pelican, Markdown, Jinja, Bootstrap, JQuery, FontAwesome, MDBootstrap
  </div>
</footer> -->
</body>
</html>